Vector operations involve vector addition, vector subtraction and vector scaling. Since each vector can hold large number of elements, the different operation can be executed in parallel. Graphic processing unit’s power can be used to compute these operation parallel at a very high rate. The vector operations are performed on the GPU in two different implementation i.e. with and without the use of page locked memory. The execution of CPU and GPU implementation of the code are compared, which gives the speedup as well as the error.

Each vector operation i.e. addition, subtraction and scaling are defined as separate functions individually for the CPU and GPU implementation.
In vector addition CPU implementation, the output vector is the sum of the two input vector. The vector elements are accessed one by one sequentially and the addition operation is performed to obtain the required results.
In vector subtraction CPU implementation, the output vector is the difference of the two input vector. The vector elements are accessed one by one sequentially and the subtraction operation is performed to obtain the required results.
In vector scaling CPU implementation, the output vector is the scaled up version of the input vector i.e. each individual element of the vector are multiplied by a scaling factor and stored in the output. In this case too, the vector elements are accesses sequentially.
For GPU implementation single dimensional grid and block is used in this case. The block dimension is set to be at 1024, the number of grid clearly depends on the size of the vector. The grid size is allocated dynamically as the ratio of the vector size and block dimension added together divided by the block dimension.
In vector addition, subtraction well as scaling GPU implementation, a number of addition operations can be run in parallel at a time which should result in faster speedup. However this is not the case, the CPU implementation is faster on non-page lock memory implementation and the GPU implementation is faster on the page lock memory implementation. The reason for this is discussed in the result section of this report.
In page locked memory implementation since the pinned memory is used, the cuda memory copy operations need not defined while running the device kernels.
The CPU and the Device implementations are called in the main function and the computation time for both the implementation for similar operations are computed. This computation can be used to study the performance limitations of GPU and CPU and also show how different optimization affect the GPU and CPU performance.
