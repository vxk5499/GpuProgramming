Gauss Jordan elimination is used to solve equations to obtain solutions. In this the equations to be solved are mapped into a matrix and then scaling and elementary row operations are performed to reduce the matrix into reduced row echelon form. In this if the matrix is square and full rank, then identity matrix, in case of augmented matrix, the last column of the matrix represents the solution. In this the Gauss Jordan Elimination is implemented in GPU and CPU, their respective computation time, error and speedup are calculated.

I. Gauss Jordan Elimination CPU implementation:
In this, initially the values in the input matrix are copied into the output matrix. After this each row of the output matrix is accessed one by one and Gauss Jordan Elimination operation is performed one by one. The number of steps required to arrive at solutions consists of performing the scaling and reduction operation a number of times equal to the number of rows in the matrix. The input to the matrix is random float value between zero and one inclusive.
Each row is accessed one by one. At first row access the first element of the row is scaled in such a way that it becomes one, it is to be noted that all elements of the row are scaled by the same value. Next, in the reduction operation the column values below the first element of the first row is made zero. In a similar fashion the loop continues till the last row of the matrix. Thus the reduced row echelon form is obtained.
The output matrix obtained will be an identity matrix in case the input matrix is square and full rank. In case of augmented matrix, the solution required can be obtained at the last column of the matrix. The computation time is calculated using the clock.


II. Gauss Jordan Elimination GPU implementation:
In case of Gauss Jordan Elimination it can be easily understood that each iteration depends on the previous iteration, thus it cannot be fully parallelizable. In GPU implementation, two separate kernels are used, one kernel for scaling the row elements and the other kernel that performs reductions on the matrix. In this case the two kernel are executed one after the other in a “for loop” for a number of times equal to the number of rows in the matrix.
In the kernel, the reads are done from the input matrix and the writes are done in the output matrix. This way race conditions can be avoided as all the elements are accessed in parallel. After each kernel execution the input matrix is updated with the values from the output matrix using the cudaMemcpy device to device function so that the next step with have the updated matrix. In both the kernels the row and column global index are calculated using the blockId, tile width and threadId. Here since large matrix cannot fit in a block, the matrix is divided into a number of tiles depending on the tile_width into different blocks. In this case a tile width of 16 is used which makes the block dim as 16*16. The grid size dynamically changes depending on the number of rows and number of columns. In this case since threads per block is 256, the multiprocessor can be effectively used if required as each multiprocessor supports 1536 threads.

A. Scaling Kernel:
Since the kernel function is executed inside a for loop, the current row is passed on as iterNo to the kernel function. In this case only if the element is in row equal to the iterNo, the element is scaled by iterNo element in that row i.e. for row 0 the scale factor for all elements in that row would be the zeroth element in that row, for the row 1 the scale factor for all elements in that row, the scale factor would be the first element and so on. The scaling of all elements in that particular row in done in parallel.
B. Reduction Kernel:
In this kernel the elementary row operations are performed to obtain zeros above and below the row element which acted as scale factor for that particular row. In this case, the current operating row is passed on as iterNo. So the elementary operations are performed in all the elements of the matrix except the elements in the row iterNo. In this case each element is the element itself subtracted by the product of the element i.e. to be made zero in that row and the corresponding element to the current operating element in iterNo row. In this all the required elementary operations are done in parallel.
Once the scaling and kernel calls are made equal to the number of row in that matrix, the required solution can be obtained.
Once the GPU and CPU solutions are obtained, they are compared to find the error between the CPU and GPU implementation. The speedup is obtained by calculating the ratio of the GPU computation time to the CPU computation time.